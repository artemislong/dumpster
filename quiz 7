# log.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import sum, col, regexp_extract, substring, concat, lit, round

spark = SparkSession.builder.appName("log.py").getOrCreate()
df = spark.read.options(delimited=' ').csv("/user/artem/access.log").toDF("log")

# Creating DataFrame of Error Code, HTTP Type, and IP addresses per row
# Define regex patterns and use regexp_extract to create columns
df = df.withColumn("IP", regexp_extract(df["log"], r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b', 0))
df = df.withColumn("type", regexp_extract(df["log"], r'[A-Z][A-Z]+', 0))
df = df.withColumn("code", regexp_extract(df["log"], r'(?<=\s)\d{3}(?=\s)', 0))
df = df.withColumn("incode", substring(col("code"), 1, 1))
df = df.withColumn("count", lit(1))
df = df.drop("log") # drop log column
columns = df.columns
# Limit the execution to the first 20 rows
#df = df.limit(10000)
# Show the resulting DataFrame
# df.show(truncate=False)


# Q4
# print("question 4: client errors")

# df4 = df.filter(df["incode"]=="4").groupBy("IP").sum("count").sort(col("sum(count)").desc())
# df4.show(truncate=False)

# Q5
print("question 5: percentage of request types")
total_count = df.select(sum("count")).collect()[0][0]
# print("total count", total_count)
# df5 = df.groupBy("type").sum("count").sort(col("sum(count)").desc())
# df5 = df5.withColumn("percentage", concat(round((col("sum(count)") / total_count) * 100, 2).cast("string"),lit("%")))
# df5.show(truncate=False)

# Q6
print("question 6: percentage of code types")
df6 = df.groupBy("incode").sum("count").sort(col("sum(count)").desc())
df6 = df6.withColumn("percentage", concat(round((col("sum(count)") / total_count) * 100, 2).cast("string"),lit("%")))
df6.show(truncate=False)
