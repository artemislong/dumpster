# log.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, regexp_extract, substring, lit

spark = SparkSession.builder.appName("log.py").getOrCreate()
df = spark.read.options(delimited=' ').csv("/user/artem/access.log").toDF("log")

# Creating DataFrame of Error Code, HTTP Type, and IP addresses per row
# Define regex patterns and use regexp_extract to create columns
df = df.withColumn("IP", regexp_extract(df["log"], r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b', 0))
df = df.withColumn("type", regexp_extract(df["log"], r'[A-Z][A-Z]+', 0))
df = df.withColumn("code", regexp_extract(df["log"], r'(?<=\s)\d{3}(?=\s)', 0))
df = df.withColumn("incode", substring(col("code"), 1, 1))
df = df.withColumn("count", lit(1))
df = df.drop("log") # drop log column
columns = df.columns
# Limit the execution to the first 20 rows
df = df.limit(100)
# Show the resulting DataFrame
df.show(truncate=False)


# Q4
print("question 4: client errors")

df4 = df.filter(df["incode"]=="4")
# Convert DataFrame to RDD, perform reduction using the reduce method, and convert back to DataFrame
df4 = df4.rdd.map(lambda row: ((row["IP"], row["incode"]), row["count"])).reduceByKey(lambda x, y: x + y)
# Convert the result RDD back to a DataFrame with appropriate columns
df4 = df4.map(lambda x: (x[0][0], x[0][1], x[1])).toDF(["IP", "incode", "count"])
df4.show(truncate=False)
